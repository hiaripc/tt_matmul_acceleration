{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 15:46:30.203 | DEBUG    | ttnn:<module>:82 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/bach/.cache/ttnn,model_cache_path=/home/bach/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n",
      "2025-01-31 15:46:31.141 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.pearson_correlation_coefficient be migrated to C++?\n",
      "2025-01-31 15:46:31.142 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n",
      "2025-01-31 15:46:31.143 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2025-01-31 15:46:31.144 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.unsqueeze_to_4D be migrated to C++?\n",
      "2025-01-31 15:46:31.145 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_torch be migrated to C++?\n",
      "2025-01-31 15:46:31.146 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_torch be migrated to C++?\n",
      "2025-01-31 15:46:31.146 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_device be migrated to C++?\n",
      "2025-01-31 15:46:31.147 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_device be migrated to C++?\n",
      "2025-01-31 15:46:31.148 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.allocate_tensor_on_device be migrated to C++?\n",
      "2025-01-31 15:46:31.148 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.copy_host_to_device_tensor be migrated to C++?\n",
      "2025-01-31 15:46:31.149 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.deallocate be migrated to C++?\n",
      "2025-01-31 15:46:31.149 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.reallocate be migrated to C++?\n",
      "2025-01-31 15:46:31.150 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.load_tensor be migrated to C++?\n",
      "2025-01-31 15:46:31.150 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.dump_tensor be migrated to C++?\n",
      "2025-01-31 15:46:31.151 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.as_tensor be migrated to C++?\n",
      "2025-01-31 15:46:31.154 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv_transpose2d be migrated to C++?\n",
      "2025-01-31 15:46:31.157 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2025-01-31 15:46:31.158 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                 Device\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Opening user mode device driver\n",
      "\n",
      "\u001b[32m2025-01-31 15:46:31.195\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.30.0, IOMMU: disabled\n",
      "\u001b[32m2025-01-31 15:46:31.196\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected PCI devices: [0]\n",
      "\u001b[32m2025-01-31 15:46:31.196\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using local chip ids: {0} and remote chip ids {}\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1000 MHz\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Profiler started on device 0\n"
     ]
    }
   ],
   "source": [
    "import ttnn \n",
    "import torch\n",
    "import time \n",
    "\n",
    "dim = 2048\n",
    "num_iters = 100\n",
    "device = ttnn.open_device(device_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002329587936401367\n"
     ]
    }
   ],
   "source": [
    "dim = 512\n",
    "a = torch.randn((dim, dim)).bfloat16()\n",
    "a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device, memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "a_t = ttnn.to_device(a_t, device=device)\n",
    "start = time.time()\n",
    "a_t = ttnn.tilize(a_t)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from_torch(ROW_MAJOR) -> to_device() = from_torch(ROW_MAJOR, device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016105008125305176 0.0015828204154968262\n"
     ]
    }
   ],
   "source": [
    "tot_from_torch_to_dev = 0\n",
    "tot_from_torch_dev = 0\n",
    "\n",
    "for i in range(num_iters):\n",
    "\n",
    "    # from_torch(ROW_MAJOR) -> to_device()\n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "    start = time.time()\n",
    "    a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "    a_t = ttnn.to_device(a_t, device=device)\n",
    "    tot_from_torch_to_dev += time.time() - start\n",
    "    \n",
    "    # from_torch(ROW_MAJOR, device)\n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "    start = time.time()\n",
    "    a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device)\n",
    "    tot_from_torch_dev += time.time() - start\n",
    "    \n",
    "tot_from_torch_to_dev /= num_iters\n",
    "tot_from_torch_dev /= num_iters\n",
    "\n",
    "print(tot_from_torch_to_dev, tot_from_torch_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tilize(DRAM) = tilize(L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00423729658126831 0.004347784519195557\n"
     ]
    }
   ],
   "source": [
    "tot_tilize_dram = 0\n",
    "tot_tilize_l1 = 0\n",
    "\n",
    "for i in range(num_iters):\n",
    "    \n",
    "    # tilize(DRAM)\n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "    a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device, memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "    a_t = ttnn.to_device(a_t, device=device)\n",
    "    start = time.time()\n",
    "    a_t = ttnn.tilize(a_t)\n",
    "    tot_tilize_dram += time.time() - start\n",
    "    \n",
    "    # tilize(L1)\n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "    a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "    a_t = ttnn.to_device(a, device=device)\n",
    "    start = time.time()\n",
    "    a_t = ttnn.tilize(a_t)\n",
    "    tot_tilize_l1 += time.time() - start\n",
    "    \n",
    "tot_tilize_dram /= num_iters\n",
    "tot_tilize_l1 /= num_iters\n",
    "\n",
    "print(tot_tilize_dram, tot_tilize_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matmul (DRAM) > matmul (L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006408035755157471 0.005533232688903809\n"
     ]
    }
   ],
   "source": [
    "tot_matmul_dram = 0\n",
    "tot_matmul_l1 = 0\n",
    "\n",
    "for i in range(num_iters):\n",
    "    \n",
    "    # matmul(DRAM)\n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "    a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device, memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "    a_t = ttnn.to_device(a_t, device=device)\n",
    "    a_t = ttnn.tilize(a_t)\n",
    "\n",
    "    b = torch.randn((dim, dim)).bfloat16()\n",
    "    b_t = ttnn.from_torch(b, layout=ttnn.ROW_MAJOR_LAYOUT, device=device, memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "    b_t = ttnn.to_device(b_t, device=device)\n",
    "    b_t = ttnn.tilize(b_t)\n",
    "\n",
    "    start = time.time()\n",
    "    c_t = ttnn.matmul(a_t, b_t, memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "    tot_matmul_dram += time.time() - start\n",
    "    \n",
    "    # matmul(L1)\n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "    a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "    a_t = ttnn.to_device(a_t, device=device)\n",
    "    a_t = ttnn.tilize(a_t)\n",
    "    \n",
    "    b = torch.randn((dim, dim)).bfloat16()\n",
    "    b_t = ttnn.from_torch(b, layout=ttnn.ROW_MAJOR_LAYOUT, device=device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "    b_t = ttnn.to_device(b_t, device=device)\n",
    "    b_t = ttnn.tilize(b_t)\n",
    "\n",
    "    start = time.time()\n",
    "    c_t = ttnn.matmul(a_t, b_t, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "    tot_matmul_l1 += time.time() - start\n",
    "    \n",
    "tot_matmul_dram /= num_iters\n",
    "tot_matmul_l1 /= num_iters\n",
    "\n",
    "print(tot_matmul_dram, tot_matmul_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tilize on device <<< on host, but dosn't work with sharded memory configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((dim, dim)).bfloat16()\n",
    "\n",
    "in0_memory_config = ttnn.create_sharded_memory_config(\n",
    "    (1, 1, dim, dim),\n",
    "    core_grid=ttnn.CoreGrid(y=8, x=8),\n",
    "    strategy=ttnn.ShardStrategy.BLOCK,\n",
    "    orientation=ttnn.ShardOrientation.ROW_MAJOR,\n",
    ")\n",
    "dtype=ttnn.DataType.BFLOAT16\n",
    "\n",
    "c= ttnn.from_torch(\n",
    "    a,\n",
    "    tile=ttnn.Tile((32, 32)),\n",
    "    # dtype=dtype,\n",
    "    layout=ttnn.ROW_MAJOR_LAYOUT,\n",
    "    device=device,\n",
    "    # memory_config=in0_memory_config\n",
    ")\n",
    "\n",
    "\n",
    "c = ttnn.to_layout(c, layout=ttnn.TILE_LAYOUT)#, memory_config = in0_memory_config)\n",
    "# d = ttnn.tilize(c)\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: ttnn._ttnn.operations.core.to_layout_t, tensor: ttnn._ttnn.tensor.Tensor, layout: ttnn._ttnn.tensor.Layout, dtype: Optional[ttnn._ttnn.tensor.DataType] = None, memory_config: Optional[ttnn._ttnn.tensor.MemoryConfig] = None, device: ttnn._ttnn.device.Device = None) -> ttnn._ttnn.tensor.Tensor\n    2. (self: ttnn._ttnn.operations.core.to_layout_t, tensor: ttnn._ttnn.tensor.Tensor, layout: ttnn._ttnn.tensor.Layout, dtype: Optional[ttnn._ttnn.tensor.DataType] = None, memory_config: Optional[ttnn._ttnn.tensor.MemoryConfig] = None, device: ttnn._ttnn.multi_device.MeshDevice = None) -> ttnn._ttnn.tensor.Tensor\n\nInvoked with: <ttnn._ttnn.operations.core.to_layout_t object at 0x7fcf98890ef0>, ttnn.Tensor([[ 0.27930, -0.09619,  ..., -0.24805,  0.15820],\n             [-0.38867,  0.91797,  ...,  2.59375, -0.43359],\n             ...,\n             [-0.20508,  0.45312,  ...,  1.17969,  1.07031],\n             [-1.24219,  1.66406,  ..., -0.05640,  0.49609]], shape=Shape([2048, 2048]), dtype=DataType::BFLOAT16, layout=Layout::ROW_MAJOR); kwargs: device=<ttnn._ttnn.device.Device object at 0x7fcf7e739730>, memory_config=MemoryConfig(memory_layout=TensorMemoryLayout::BLOCK_SHARDED,buffer_type=BufferType::L1,shard_spec=ShardSpec(grid={[(x=0,y=0) - (x=7,y=7)]},shape={128, 128},orientation=ShardOrientation::ROW_MAJOR,halo=0,mode=ShardMode::PHYSICAL,physical_shard_shape=std::nullopt))\n\nDid you forget to `#include <pybind11/stl.h>`? Or <pybind11/complex.h>,\n<pybind11/functional.h>, <pybind11/chrono.h>, etc. Some automatic\nconversions are optional and require extra headers to be included\nwhen compiling your pybind11 module.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# d = ttnn.tilize(c)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[43mttnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min0_memory_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     tot_tilize \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     22\u001b[0m tot_tilize_host \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m num_iters\n",
      "File \u001b[0;32m~/tt-install/tt-metal/ttnn/ttnn/decorators.py:329\u001b[0m, in \u001b[0;36mFastOperation.__call__\u001b[0;34m(self, *function_args, **function_kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mfunction_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunction_kwargs):\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunction_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunction_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: ttnn._ttnn.operations.core.to_layout_t, tensor: ttnn._ttnn.tensor.Tensor, layout: ttnn._ttnn.tensor.Layout, dtype: Optional[ttnn._ttnn.tensor.DataType] = None, memory_config: Optional[ttnn._ttnn.tensor.MemoryConfig] = None, device: ttnn._ttnn.device.Device = None) -> ttnn._ttnn.tensor.Tensor\n    2. (self: ttnn._ttnn.operations.core.to_layout_t, tensor: ttnn._ttnn.tensor.Tensor, layout: ttnn._ttnn.tensor.Layout, dtype: Optional[ttnn._ttnn.tensor.DataType] = None, memory_config: Optional[ttnn._ttnn.tensor.MemoryConfig] = None, device: ttnn._ttnn.multi_device.MeshDevice = None) -> ttnn._ttnn.tensor.Tensor\n\nInvoked with: <ttnn._ttnn.operations.core.to_layout_t object at 0x7fcf98890ef0>, ttnn.Tensor([[ 0.27930, -0.09619,  ..., -0.24805,  0.15820],\n             [-0.38867,  0.91797,  ...,  2.59375, -0.43359],\n             ...,\n             [-0.20508,  0.45312,  ...,  1.17969,  1.07031],\n             [-1.24219,  1.66406,  ..., -0.05640,  0.49609]], shape=Shape([2048, 2048]), dtype=DataType::BFLOAT16, layout=Layout::ROW_MAJOR); kwargs: device=<ttnn._ttnn.device.Device object at 0x7fcf7e739730>, memory_config=MemoryConfig(memory_layout=TensorMemoryLayout::BLOCK_SHARDED,buffer_type=BufferType::L1,shard_spec=ShardSpec(grid={[(x=0,y=0) - (x=7,y=7)]},shape={128, 128},orientation=ShardOrientation::ROW_MAJOR,halo=0,mode=ShardMode::PHYSICAL,physical_shard_shape=std::nullopt))\n\nDid you forget to `#include <pybind11/stl.h>`? Or <pybind11/complex.h>,\n<pybind11/functional.h>, <pybind11/chrono.h>, etc. Some automatic\nconversions are optional and require extra headers to be included\nwhen compiling your pybind11 module."
     ]
    }
   ],
   "source": [
    "tot_tilize_host = 0\n",
    "tot_tilize = 0\n",
    "tot_move = 0\n",
    "for i in range(num_iters):\n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "\n",
    "    start = time.time()\n",
    "    c = ttnn.from_torch(a, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "    tot_tilize_host = time.time() - start\n",
    "    \n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "\n",
    "    start = time.time()\n",
    "    c = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device)\n",
    "    tot_move = time.time() - start\n",
    "    \n",
    "    start = time.time()\n",
    "    # d = ttnn.tilize(c)\n",
    "    d = ttnn.to_layout(c, device=device, memory_config=in0_memory_config)\n",
    "    tot_tilize = time.time() - start\n",
    "\n",
    "tot_tilize_host /= num_iters\n",
    "tot_tilize /= num_iters\n",
    "tot_move /= num_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003919928073883057"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_tilize + tot_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003919928073883057"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_tilize + tot_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003970417976379394"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_tilize_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dict.fromkeys([1])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ttnn.Tensor([[73.50000, -39.50000,  ..., -4.96875, 79.00000],\n",
      "             [-57.25000, 22.62500,  ..., 36.50000, -46.25000],\n",
      "             ...,\n",
      "             [-61.75000, 26.12500,  ..., -77.00000, -30.62500],\n",
      "             [-32.25000, -51.75000,  ..., -27.00000, 115.00000]], shape=Shape([2048, 2048]), dtype=DataType::BFLOAT16, layout=Layout::TILE)\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Closing device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Disabling and clearing program cache on device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1000 MHz\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Profiler started on device 0\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((dim, dim)).bfloat16()\n",
    "b = torch.randn((dim, dim)).bfloat16()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "a_t = ttnn.from_torch(a, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "b_t = ttnn.from_torch(b, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "c_1 = ttnn.matmul(a_t, b_t)\n",
    "tot_tilize_host = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device)\n",
    "a_t = ttnn.tilize(a_t)\n",
    "b_t = ttnn.from_torch(b, layout=ttnn.ROW_MAJOR_LAYOUT, device=device)\n",
    "b_t = ttnn.tilize(b_t)\n",
    "c_2 = ttnn.matmul(a_t, b_t) \n",
    "tot_move = time.time() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.791964054107666, 0.011814594268798828)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_tilize_host, tot_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0024967384338378906"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = torch.randn((dim, dim)).bfloat16()\n",
    "num_iters = 100\n",
    "start = time.time()\n",
    "for i in range(num_iters):\n",
    "    c = ttnn.to_device(ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT), device=device)\n",
    "t_avg = (time.time() - start)/num_iters\n",
    "t_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.01391284629646"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(dim*dim*16) / (t_avg * 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0025472283363342286"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(num_iters):\n",
    "    # a = torch.randn((dim, dim)).bfloat16()\n",
    "    c = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device)\n",
    "t_avg = (time.time() - start)/num_iters\n",
    "t_avg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tt-menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
