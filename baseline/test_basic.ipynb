{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 12:13:39.630 | DEBUG    | ttnn:<module>:82 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/bach/.cache/ttnn,model_cache_path=/home/bach/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n",
      "2025-02-28 12:13:40.724 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.pearson_correlation_coefficient be migrated to C++?\n",
      "2025-02-28 12:13:40.726 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n",
      "2025-02-28 12:13:40.727 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2025-02-28 12:13:40.728 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.unsqueeze_to_4D be migrated to C++?\n",
      "2025-02-28 12:13:40.728 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_torch be migrated to C++?\n",
      "2025-02-28 12:13:40.729 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_torch be migrated to C++?\n",
      "2025-02-28 12:13:40.729 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_device be migrated to C++?\n",
      "2025-02-28 12:13:40.730 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_device be migrated to C++?\n",
      "2025-02-28 12:13:40.731 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.allocate_tensor_on_device be migrated to C++?\n",
      "2025-02-28 12:13:40.731 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.copy_host_to_device_tensor be migrated to C++?\n",
      "2025-02-28 12:13:40.732 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.deallocate be migrated to C++?\n",
      "2025-02-28 12:13:40.732 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.reallocate be migrated to C++?\n",
      "2025-02-28 12:13:40.733 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.load_tensor be migrated to C++?\n",
      "2025-02-28 12:13:40.733 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.dump_tensor be migrated to C++?\n",
      "2025-02-28 12:13:40.735 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.as_tensor be migrated to C++?\n",
      "2025-02-28 12:13:40.738 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv_transpose2d be migrated to C++?\n",
      "2025-02-28 12:13:40.742 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2025-02-28 12:13:40.743 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                 Device\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Opening user mode device driver\n",
      "\n",
      "\u001b[32m2025-02-28 12:13:40.839\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.30.0, IOMMU: disabled\n",
      "\u001b[32m2025-02-28 12:13:40.840\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected PCI devices: [0]\n",
      "\u001b[32m2025-02-28 12:13:40.840\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using local chip ids: {0} and remote chip ids {}\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1000 MHz\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Profiler started on device 0\n"
     ]
    }
   ],
   "source": [
    "import ttnn \n",
    "import torch\n",
    "import time \n",
    "\n",
    "dim = 2048\n",
    "num_iters = 100\n",
    "device = ttnn.open_device(device_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping operations on set of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 512\n",
    "a = torch.randn((dim, dim))\n",
    "a_t = ttnn.from_torch(a, layout=ttnn.TILE_LAYOUT, device=device, memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "\n",
    "b = torch.randn((dim, dim))\n",
    "b_t = ttnn.from_torch(b, layout=ttnn.TILE_LAYOUT, device=device, memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "\n",
    "d = torch.randn((dim, dim))\n",
    "d_t = ttnn.from_torch(d, layout=ttnn.TILE_LAYOUT, device=device, memory_config=ttnn.DRAM_MEMORY_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{[(x=0,y=0) - (x=2,y=4)]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_grid=ttnn.CoreRangeSet({\n",
    "            ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(2, 4)),\n",
    "        })\n",
    "core_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ttnn.CoreGrid(x=2, y=4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_grid=ttnn.CoreGrid(x=2, y=4)\n",
    "core_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: ttnn._ttnn.operations.matmul.matmul_t, input_tensor_a: ttnn._ttnn.tensor.Tensor, input_tensor_b: ttnn._ttnn.tensor.Tensor, *, transpose_a: bool = False, transpose_b: bool = False, memory_config: Optional[ttnn._ttnn.tensor.MemoryConfig] = None, dtype: Optional[ttnn._ttnn.tensor.DataType] = None, program_config: Optional[Union[ttnn::operations::matmul::MatmulMultiCoreProgramConfig, ttnn::operations::matmul::MatmulMultiCoreNonOptimizedReuseProgramConfig, ttnn._ttnn.operations.matmul.MatmulMultiCoreReuseProgramConfig, ttnn._ttnn.operations.matmul.MatmulMultiCoreReuseMultiCastProgramConfig, ttnn._ttnn.operations.matmul.MatmulMultiCoreReuseMultiCast1DProgramConfig, ttnn._ttnn.operations.matmul.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig]] = None, activation: Optional[str] = None, compute_kernel_config: Optional[Union[ttnn._ttnn.operations.core.GrayskullComputeKernelConfig, ttnn._ttnn.operations.core.WormholeComputeKernelConfig]] = None, core_grid: Optional[ttnn._ttnn.types.CoreGrid] = None, output_tile: Optional[ttnn._ttnn.tensor.Tile] = None, optional_output_tensor: Optional[ttnn._ttnn.tensor.Tensor] = None) -> ttnn._ttnn.tensor.Tensor\n\nInvoked with: <ttnn._ttnn.operations.matmul.matmul_t object at 0x7fe096c2d370>, ttnn.Tensor([[ 2.15855, -0.01651,  ...,  0.76436, -0.86792],\n             [-1.36429, -1.21681,  ...,  0.01449, -1.11365],\n             ...,\n             [-1.45339,  0.70382,  ..., -0.65734,  0.76216],\n             [-0.81560, -0.79583,  ...,  1.11969,  0.63435]], shape=Shape([512, 512]), dtype=DataType::FLOAT32, layout=Layout::TILE), ttnn.Tensor([[ 0.87252,  1.61767,  ...,  1.11987,  0.87598],\n             [-0.64968,  0.13602,  ..., -1.42009, -0.97694],\n             ...,\n             [ 1.21289,  0.03902,  ...,  0.92751,  0.31833],\n             [ 0.86525, -0.21949,  ..., -0.58745,  0.94282]], shape=Shape([512, 512]), dtype=DataType::FLOAT32, layout=Layout::TILE); kwargs: core_grid={[(x=0,y=0) - (x=2,y=4)]}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iters):\n\u001b[0;32m----> 7\u001b[0m     c_t \u001b[38;5;241m=\u001b[39m \u001b[43mttnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43ma_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mb_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcore_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mttnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCoreRangeSet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mttnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCoreRange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mttnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCoreCoord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mttnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCoreCoord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     e_t \u001b[38;5;241m=\u001b[39m ttnn\u001b[38;5;241m.\u001b[39mmatmul(\n\u001b[1;32m     16\u001b[0m         c_t, \n\u001b[1;32m     17\u001b[0m         d_t,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m         })\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     23\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/tt-install/tt-metal/ttnn/ttnn/decorators.py:329\u001b[0m, in \u001b[0;36mFastOperation.__call__\u001b[0;34m(self, *function_args, **function_kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mfunction_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunction_kwargs):\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunction_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunction_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: ttnn._ttnn.operations.matmul.matmul_t, input_tensor_a: ttnn._ttnn.tensor.Tensor, input_tensor_b: ttnn._ttnn.tensor.Tensor, *, transpose_a: bool = False, transpose_b: bool = False, memory_config: Optional[ttnn._ttnn.tensor.MemoryConfig] = None, dtype: Optional[ttnn._ttnn.tensor.DataType] = None, program_config: Optional[Union[ttnn::operations::matmul::MatmulMultiCoreProgramConfig, ttnn::operations::matmul::MatmulMultiCoreNonOptimizedReuseProgramConfig, ttnn._ttnn.operations.matmul.MatmulMultiCoreReuseProgramConfig, ttnn._ttnn.operations.matmul.MatmulMultiCoreReuseMultiCastProgramConfig, ttnn._ttnn.operations.matmul.MatmulMultiCoreReuseMultiCast1DProgramConfig, ttnn._ttnn.operations.matmul.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig]] = None, activation: Optional[str] = None, compute_kernel_config: Optional[Union[ttnn._ttnn.operations.core.GrayskullComputeKernelConfig, ttnn._ttnn.operations.core.WormholeComputeKernelConfig]] = None, core_grid: Optional[ttnn._ttnn.types.CoreGrid] = None, output_tile: Optional[ttnn._ttnn.tensor.Tile] = None, optional_output_tensor: Optional[ttnn._ttnn.tensor.Tensor] = None) -> ttnn._ttnn.tensor.Tensor\n\nInvoked with: <ttnn._ttnn.operations.matmul.matmul_t object at 0x7fe096c2d370>, ttnn.Tensor([[ 2.15855, -0.01651,  ...,  0.76436, -0.86792],\n             [-1.36429, -1.21681,  ...,  0.01449, -1.11365],\n             ...,\n             [-1.45339,  0.70382,  ..., -0.65734,  0.76216],\n             [-0.81560, -0.79583,  ...,  1.11969,  0.63435]], shape=Shape([512, 512]), dtype=DataType::FLOAT32, layout=Layout::TILE), ttnn.Tensor([[ 0.87252,  1.61767,  ...,  1.11987,  0.87598],\n             [-0.64968,  0.13602,  ..., -1.42009, -0.97694],\n             ...,\n             [ 1.21289,  0.03902,  ...,  0.92751,  0.31833],\n             [ 0.86525, -0.21949,  ..., -0.58745,  0.94282]], shape=Shape([512, 512]), dtype=DataType::FLOAT32, layout=Layout::TILE); kwargs: core_grid={[(x=0,y=0) - (x=2,y=4)]}"
     ]
    }
   ],
   "source": [
    "## a x b = c -> c x d = e\n",
    "## grid (4x2) -> (4x2)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(iters):\n",
    "    c_t = ttnn.matmul(\n",
    "        a_t, \n",
    "        b_t,\n",
    "        core_grid=ttnn.CoreRangeSet({\n",
    "            ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(2, 4)),\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    e_t = ttnn.matmul(\n",
    "        c_t, \n",
    "        d_t,\n",
    "        core_grid=ttnn.CoreRangeSet({\n",
    "            ttnn.CoreRange(ttnn.CoreCoord(0, 4), ttnn.CoreCoord(2, 8)),\n",
    "        })\n",
    "    )\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a x b = c -> c x d = e\n",
    "## grid (4x4)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(iters):\n",
    "    c_t = ttnn.matmul(\n",
    "        a_t, \n",
    "        b_t,\n",
    "        core_grid=ttnn.CoreRangeSet({\n",
    "            ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(2, 8)),\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    e_t = ttnn.matmul(\n",
    "        c_t, \n",
    "        d_t,\n",
    "        core_grid=ttnn.CoreRangeSet({\n",
    "            ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(2, 8)),\n",
    "        })\n",
    "    )\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 vs DRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 8192\n",
    "a = torch.randn((dim, dim))\n",
    "a_t = ttnn.from_torch(a, layout=ttnn.TILE_LAYOUT, device=device, memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "\n",
    "b = torch.randn((dim, dim))\n",
    "b_t = ttnn.from_torch(b, layout=ttnn.TILE_LAYOUT, device=device, memory_config=ttnn.DRAM_MEMORY_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  38.116708517074585\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    c = ttnn.matmul(\n",
    "        a_t, \n",
    "        b_t,\n",
    "        dtype=ttnn.bfloat16,\n",
    "        core_grid=ttnn.CoreGrid(y=8, x=8),\n",
    "    )\n",
    "end = time.time()\n",
    "print(\"Time: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from_torch(ROW_MAJOR) -> to_device() = from_torch(ROW_MAJOR, device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00044506072998046876 0.00038550615310668947\n"
     ]
    }
   ],
   "source": [
    "tot_from_torch_to_dev = 0\n",
    "tot_from_torch_dev = 0\n",
    "\n",
    "for i in range(num_iters):\n",
    "\n",
    "    # from_torch(ROW_MAJOR) -> to_device()\n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "    start = time.time()\n",
    "    a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "    a_t = ttnn.to_device(a_t, device=device)\n",
    "    tot_from_torch_to_dev += time.time() - start\n",
    "    \n",
    "    # from_torch(ROW_MAJOR, device)\n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "    start = time.time()\n",
    "    a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device)\n",
    "    tot_from_torch_dev += time.time() - start\n",
    "    \n",
    "tot_from_torch_to_dev /= num_iters\n",
    "tot_from_torch_dev /= num_iters\n",
    "\n",
    "print(tot_from_torch_to_dev, tot_from_torch_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor on device: tilize() = to_layout() (first time not??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0028214049339294433 0.002933976650238037\n"
     ]
    }
   ],
   "source": [
    "tot_tilize = 0\n",
    "tot_to_layout = 0\n",
    "\n",
    "for i in range(num_iters):\n",
    "    \n",
    "    # tilize(DRAM)\n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "    a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device, memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "    a_t = ttnn.to_device(a_t, device=device)\n",
    "    start = time.time()\n",
    "    a_t = ttnn.tilize(a_t)\n",
    "    tot_tilize += time.time() - start\n",
    "    \n",
    "    # to_layout\n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "    a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "    a_t = ttnn.to_device(a_t, device=device)\n",
    "    start = time.time()\n",
    "    a_t = ttnn.to_layout(a_t, layout=ttnn.TILE_LAYOUT)\n",
    "    tot_to_layout += time.time() - start\n",
    "    \n",
    "tot_tilize /= num_iters\n",
    "tot_to_layout /= num_iters\n",
    "\n",
    "print(tot_tilize, tot_to_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tilize(DRAM) = tilize(L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00423729658126831 0.004347784519195557\n"
     ]
    }
   ],
   "source": [
    "tot_tilize_dram = 0\n",
    "tot_tilize_l1 = 0\n",
    "\n",
    "for i in range(num_iters):\n",
    "    \n",
    "    # tilize(DRAM)\n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "    a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device, memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "    a_t = ttnn.to_device(a_t, device=device)\n",
    "    start = time.time()\n",
    "    a_t = ttnn.tilize(a_t)\n",
    "    tot_tilize_dram += time.time() - start\n",
    "    \n",
    "    # tilize(L1)\n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "    a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "    a_t = ttnn.to_device(a, device=device)\n",
    "    start = time.time()\n",
    "    a_t = ttnn.tilize(a_t)\n",
    "    tot_tilize_l1 += time.time() - start\n",
    "    \n",
    "tot_tilize_dram /= num_iters\n",
    "tot_tilize_l1 /= num_iters\n",
    "\n",
    "print(tot_tilize_dram, tot_tilize_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matmul (DRAM) > matmul (L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0063421630859375 0.005454301834106445\n"
     ]
    }
   ],
   "source": [
    "tot_matmul_dram = 0\n",
    "tot_matmul_l1 = 0\n",
    "\n",
    "dim = 2048\n",
    "\n",
    "for i in range(num_iters):\n",
    "    \n",
    "    # matmul(DRAM)\n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "    a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device, memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "    a_t = ttnn.to_device(a_t, device=device)\n",
    "    a_t = ttnn.tilize(a_t)\n",
    "\n",
    "    b = torch.randn((dim, dim)).bfloat16()\n",
    "    b_t = ttnn.from_torch(b, layout=ttnn.ROW_MAJOR_LAYOUT, device=device, memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "    b_t = ttnn.to_device(b_t, device=device)\n",
    "    b_t = ttnn.tilize(b_t)\n",
    "\n",
    "    start = time.time()\n",
    "    c_t = ttnn.matmul(a_t, b_t, memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "    tot_matmul_dram += time.time() - start\n",
    "    \n",
    "    # matmul(L1)\n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "    a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "    a_t = ttnn.to_device(a_t, device=device)\n",
    "    a_t = ttnn.tilize(a_t)\n",
    "    \n",
    "    b = torch.randn((dim, dim)).bfloat16()\n",
    "    b_t = ttnn.from_torch(b, layout=ttnn.ROW_MAJOR_LAYOUT, device=device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "    b_t = ttnn.to_device(b_t, device=device)\n",
    "    b_t = ttnn.tilize(b_t)\n",
    "\n",
    "    start = time.time()\n",
    "    c_t = ttnn.matmul(a_t, b_t, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "    tot_matmul_l1 += time.time() - start\n",
    "    \n",
    "tot_matmul_dram /= num_iters\n",
    "tot_matmul_l1 /= num_iters\n",
    "\n",
    "print(tot_matmul_dram, tot_matmul_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tilize on device <<< on host, but dosn't work with sharded memory configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((dim, dim)).bfloat16()\n",
    "\n",
    "in0_memory_config = ttnn.create_sharded_memory_config(\n",
    "    (1, 1, dim, dim),\n",
    "    core_grid=ttnn.CoreGrid(y=8, x=8),\n",
    "    strategy=ttnn.ShardStrategy.BLOCK,\n",
    "    orientation=ttnn.ShardOrientation.ROW_MAJOR,\n",
    ")\n",
    "dtype=ttnn.DataType.BFLOAT16\n",
    "\n",
    "c= ttnn.from_torch(\n",
    "    a,\n",
    "    tile=ttnn.Tile((32, 32)),\n",
    "    # dtype=dtype,\n",
    "    layout=ttnn.ROW_MAJOR_LAYOUT,\n",
    "    device=device,\n",
    "    # memory_config=in0_memory_config\n",
    ")\n",
    "\n",
    "\n",
    "c = ttnn.to_layout(c, layout=ttnn.TILE_LAYOUT)#, memory_config = in0_memory_config)\n",
    "# d = ttnn.tilize(c)\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: ttnn._ttnn.operations.core.to_layout_t, tensor: ttnn._ttnn.tensor.Tensor, layout: ttnn._ttnn.tensor.Layout, dtype: Optional[ttnn._ttnn.tensor.DataType] = None, memory_config: Optional[ttnn._ttnn.tensor.MemoryConfig] = None, device: ttnn._ttnn.device.Device = None) -> ttnn._ttnn.tensor.Tensor\n    2. (self: ttnn._ttnn.operations.core.to_layout_t, tensor: ttnn._ttnn.tensor.Tensor, layout: ttnn._ttnn.tensor.Layout, dtype: Optional[ttnn._ttnn.tensor.DataType] = None, memory_config: Optional[ttnn._ttnn.tensor.MemoryConfig] = None, device: ttnn._ttnn.multi_device.MeshDevice = None) -> ttnn._ttnn.tensor.Tensor\n\nInvoked with: <ttnn._ttnn.operations.core.to_layout_t object at 0x7fcf98890ef0>, ttnn.Tensor([[ 0.27930, -0.09619,  ..., -0.24805,  0.15820],\n             [-0.38867,  0.91797,  ...,  2.59375, -0.43359],\n             ...,\n             [-0.20508,  0.45312,  ...,  1.17969,  1.07031],\n             [-1.24219,  1.66406,  ..., -0.05640,  0.49609]], shape=Shape([2048, 2048]), dtype=DataType::BFLOAT16, layout=Layout::ROW_MAJOR); kwargs: device=<ttnn._ttnn.device.Device object at 0x7fcf7e739730>, memory_config=MemoryConfig(memory_layout=TensorMemoryLayout::BLOCK_SHARDED,buffer_type=BufferType::L1,shard_spec=ShardSpec(grid={[(x=0,y=0) - (x=7,y=7)]},shape={128, 128},orientation=ShardOrientation::ROW_MAJOR,halo=0,mode=ShardMode::PHYSICAL,physical_shard_shape=std::nullopt))\n\nDid you forget to `#include <pybind11/stl.h>`? Or <pybind11/complex.h>,\n<pybind11/functional.h>, <pybind11/chrono.h>, etc. Some automatic\nconversions are optional and require extra headers to be included\nwhen compiling your pybind11 module.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# d = ttnn.tilize(c)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[43mttnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min0_memory_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     tot_tilize \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     22\u001b[0m tot_tilize_host \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m num_iters\n",
      "File \u001b[0;32m~/tt-install/tt-metal/ttnn/ttnn/decorators.py:329\u001b[0m, in \u001b[0;36mFastOperation.__call__\u001b[0;34m(self, *function_args, **function_kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mfunction_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunction_kwargs):\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunction_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunction_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: ttnn._ttnn.operations.core.to_layout_t, tensor: ttnn._ttnn.tensor.Tensor, layout: ttnn._ttnn.tensor.Layout, dtype: Optional[ttnn._ttnn.tensor.DataType] = None, memory_config: Optional[ttnn._ttnn.tensor.MemoryConfig] = None, device: ttnn._ttnn.device.Device = None) -> ttnn._ttnn.tensor.Tensor\n    2. (self: ttnn._ttnn.operations.core.to_layout_t, tensor: ttnn._ttnn.tensor.Tensor, layout: ttnn._ttnn.tensor.Layout, dtype: Optional[ttnn._ttnn.tensor.DataType] = None, memory_config: Optional[ttnn._ttnn.tensor.MemoryConfig] = None, device: ttnn._ttnn.multi_device.MeshDevice = None) -> ttnn._ttnn.tensor.Tensor\n\nInvoked with: <ttnn._ttnn.operations.core.to_layout_t object at 0x7fcf98890ef0>, ttnn.Tensor([[ 0.27930, -0.09619,  ..., -0.24805,  0.15820],\n             [-0.38867,  0.91797,  ...,  2.59375, -0.43359],\n             ...,\n             [-0.20508,  0.45312,  ...,  1.17969,  1.07031],\n             [-1.24219,  1.66406,  ..., -0.05640,  0.49609]], shape=Shape([2048, 2048]), dtype=DataType::BFLOAT16, layout=Layout::ROW_MAJOR); kwargs: device=<ttnn._ttnn.device.Device object at 0x7fcf7e739730>, memory_config=MemoryConfig(memory_layout=TensorMemoryLayout::BLOCK_SHARDED,buffer_type=BufferType::L1,shard_spec=ShardSpec(grid={[(x=0,y=0) - (x=7,y=7)]},shape={128, 128},orientation=ShardOrientation::ROW_MAJOR,halo=0,mode=ShardMode::PHYSICAL,physical_shard_shape=std::nullopt))\n\nDid you forget to `#include <pybind11/stl.h>`? Or <pybind11/complex.h>,\n<pybind11/functional.h>, <pybind11/chrono.h>, etc. Some automatic\nconversions are optional and require extra headers to be included\nwhen compiling your pybind11 module."
     ]
    }
   ],
   "source": [
    "tot_tilize_host = 0\n",
    "tot_tilize = 0\n",
    "tot_move = 0\n",
    "for i in range(num_iters):\n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "\n",
    "    start = time.time()\n",
    "    c = ttnn.from_torch(a, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "    tot_tilize_host = time.time() - start\n",
    "    \n",
    "    a = torch.randn((dim, dim)).bfloat16()\n",
    "\n",
    "    start = time.time()\n",
    "    c = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device)\n",
    "    tot_move = time.time() - start\n",
    "    \n",
    "    start = time.time()\n",
    "    # d = ttnn.tilize(c)\n",
    "    d = ttnn.to_layout(c, device=device, memory_config=in0_memory_config)\n",
    "    tot_tilize = time.time() - start\n",
    "\n",
    "tot_tilize_host /= num_iters\n",
    "tot_tilize /= num_iters\n",
    "tot_move /= num_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003919928073883057"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_tilize + tot_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003919928073883057"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_tilize + tot_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003970417976379394"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_tilize_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dict.fromkeys([1])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ttnn.Tensor([[73.50000, -39.50000,  ..., -4.96875, 79.00000],\n",
      "             [-57.25000, 22.62500,  ..., 36.50000, -46.25000],\n",
      "             ...,\n",
      "             [-61.75000, 26.12500,  ..., -77.00000, -30.62500],\n",
      "             [-32.25000, -51.75000,  ..., -27.00000, 115.00000]], shape=Shape([2048, 2048]), dtype=DataType::BFLOAT16, layout=Layout::TILE)\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Closing device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Disabling and clearing program cache on device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1000 MHz\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Profiler started on device 0\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((dim, dim)).bfloat16()\n",
    "b = torch.randn((dim, dim)).bfloat16()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "a_t = ttnn.from_torch(a, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "b_t = ttnn.from_torch(b, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "c_1 = ttnn.matmul(a_t, b_t)\n",
    "tot_tilize_host = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "a_t = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device)\n",
    "a_t = ttnn.tilize(a_t)\n",
    "b_t = ttnn.from_torch(b, layout=ttnn.ROW_MAJOR_LAYOUT, device=device)\n",
    "b_t = ttnn.tilize(b_t)\n",
    "c_2 = ttnn.matmul(a_t, b_t) \n",
    "tot_move = time.time() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.791964054107666, 0.011814594268798828)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_tilize_host, tot_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0024967384338378906"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = torch.randn((dim, dim)).bfloat16()\n",
    "num_iters = 100\n",
    "start = time.time()\n",
    "for i in range(num_iters):\n",
    "    c = ttnn.to_device(ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT), device=device)\n",
    "t_avg = (time.time() - start)/num_iters\n",
    "t_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.01391284629646"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(dim*dim*16) / (t_avg * 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0025472283363342286"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(num_iters):\n",
    "    # a = torch.randn((dim, dim)).bfloat16()\n",
    "    c = ttnn.from_torch(a, layout=ttnn.ROW_MAJOR_LAYOUT, device=device)\n",
    "t_avg = (time.time() - start)/num_iters\n",
    "t_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRID selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = (1,1)\n",
    "dim = 256\n",
    "in0_shape = (dim, dim)\n",
    "in1_shape = (dim, dim)\n",
    "dtype = ttnn.bfloat16\n",
    "n_exec = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0044387316703796385\n"
     ]
    }
   ],
   "source": [
    "tot_time = 0\n",
    "for i in range(n_exec):\n",
    "    in0 = torch.ones(in0_shape).bfloat16()\n",
    "    in1 = torch.randn(in1_shape).bfloat16()\n",
    "\n",
    "    in0_t =  ttnn.from_torch(\n",
    "            in0,\n",
    "            tile=ttnn.Tile((32, 32)),\n",
    "            dtype=dtype,\n",
    "            layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "    in0_t = ttnn.to_device(\n",
    "            in0_t, \n",
    "            device=device,\n",
    "            memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "    in0_t = ttnn.tilize(in0_t)\n",
    "\n",
    "    in1_t = ttnn.from_torch(\n",
    "            in1,\n",
    "            tile=ttnn.Tile((32, 32)),\n",
    "            dtype=dtype,\n",
    "            layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "    in1_t = ttnn.to_device(\n",
    "            in1_t, \n",
    "            device=device,\n",
    "            memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "    in1_t = ttnn.tilize(in1_t)\n",
    "        \n",
    "    start = time.time()\n",
    "    in2_t = ttnn.matmul(\n",
    "        in0_t,\n",
    "        in1_t, \n",
    "        core_grid=ttnn.CoreGrid(y=grid_size[1], x=grid_size[0]),\n",
    "    )\n",
    "    tot_time += time.time() - start\n",
    "\n",
    "t_avg = tot_time/n_exec\n",
    "print(t_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tt-menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
